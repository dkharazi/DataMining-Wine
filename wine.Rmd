---
title: "Report"
author: "Darius Kharazi"
date: "10/22/2017"
output: html_document
---

## Import Statements

First, we will import some libraries that are good for fitting machine-learning models to the data. We will also import our data.

```{r import, message=FALSE, warning=FALSE}
## Import Packages
library(knitr)
library(readr)
library(car)
library(leaps)
library(caret)
library(e1071)
library(rpart)
library(neuralnet)
library(nnet)
library(randomForest)
library(C50)

## Import Datasets
wine <- read_csv("~/wine.csv") ## Insert Data Location Here
wine.df <- data.frame(wine)
wine.df <- wine.df[-c(1296,1297),]
```


## Exploratory Data Analysis

Next, we will run some exploratory analysis on the data.

```{r EDA}
## Data Summary
summary(wine.df)

## Histogram of quality
hist(wine.df$quality,
     col = "blue",
     breaks = 5,
     main = "Quality",
     xlab = "Quality")

## Histogram of quality
hist(wine.df$pH,
     col = "blue",
     breaks = 10,
     main = "pH Level",
     xlab = "pH Level")

## Histogram of alcohol
hist(wine.df$alcohol,
     col = "blue",
     breaks = 5,
     main = "Alcohol Content",
     xlab = "Alcohol")

## Histogram of sulfur dioxide
hist(wine.df$sulph,
     col = "blue",
     breaks = 20,
     main = "Sulfur Dioxide",
     xlab = "Sulfur Dioxide",
     xlim = c(0.3, 1.4))

## Histogram of total sulfur dioxide
hist(wine.df$tot_sulf_d,
     col = "blue",
     breaks = 15,
     main = "Total Sulfur Dioxide",
     xlab = "Total Sulfur Dioxide",
     xlim = c(0, 200))

# Histogram of free sulfur dioxide
hist(wine.df$free_sulf_d,
     col = "blue",
     breaks = 15,
     main = "Free Sulfur Dioxide",
     xlab = "Free Sulfur Dioxide",
     xlim = c(0, 60))
```

![histograms](/graphics/EDA1.png)

According to the pairwise plots between the continuous variables in the dataset, it seems like there aren't many outstanding variables that are correlated with quality or class. Some variables, such as "sulph," "tot_sulf_d"  and "free_sulf_d," seem to share some level of multicollinearity.  According to the summary of the "quality" variable, a large amount of the observations seems to be given a quality of 6.0, since the median is 6.0, mean is 5.64, and the 3rd quartile is 6.0. The majority of the observations seems to have a quality of around 6.0, since both the first and third quartiles are very close to the median/mean. However, the histogram of wine quality seems to be slightly skewed left, indicating the mean is on the right of the peak value. Additionally, there doesn't seem to be many wines with an alcohol content smaller than 9, but the majority of wines have an alcohol content between 9 and 10, according to the histogram, indicating a huge jump of alcohol content. According to the summary of wines with the highest alcohol contents and their corresponding boxplots, wines with the highest alcohol content in the sample seem to have higher overall qualities compared to the general sample. Furthermore, the histogram relating to wine containing an extremely high alcohol content is left skewed. However, the differing sample sizes between the wines containing a higher alcohol content and the entire sample should be noted. The majority of wines have low levels of sulphates, which can be seen in the right-skewed histogram relating to the sample's sulphate levels. With additional research, this may seem obvious, since low levels of sulphates are used in wines to maintain freshness, but large amounts of sulphates tend to reduce the quality of wine. The majority of wines have low levels of total sulfur dioxide and free sulfur dioxide, since each variable's corresponding histograms are right-skewed, which validates our previous claim about the sulphates in wine. This also could indicate some levels of multicollinearity between the three variables, which would validate our claim mentioned earlier, as well.

## Data Transformation

Now, we will transform the data in preparation for fitting regression and machine-learning models.

```{r transform}
## Change High and Low Values
wine.df[wine.df$class == "High", 14] <-  1
wine.df[wine.df$class == "Low", 14] <-  0
wine.df$class <- as.numeric(wine.df$class)
```

For modeling our data using logistic regression, the "class"" variable was slightly modified. The "high" values were assigned to the value "1," and the "low" values were assigned to "0," in order to properly fit a logistic regression model on the data. Outliers were not treated, after further examination. Outliers remained in the analysis, since they contained different values, and contributed some level of predictive power to our analysis. For example, many of the outliers contained high levels of alcohol or extremely low levels of alcohol. Since alcohol content seems to have some level of correlation with wine quality, this sometimes caused certain wines with high levels of alcohol content to be considered outliers. Additionally, observations with missing values were deleted, since our sample contained nearly 1,600 observations, and only 2 observations contained any missing values. Throughout the model selection approaches, two variables seemed to be included in every model: "alcohol" and "vol_acidity." However, since the full model had a very small R-Squared value, and since the data does not contain an overwhelmingly large amount of parameters, the full model will be primarily considered in further model development.

## Simple Linear Regression

```{r slr}
## Linear Regression (Full)
full.lm <- lm(quality~fx_acidity+vol_acidity+citric_acid+resid_sugar+chlorides+
                  free_sulf_d+tot_sulf_d+density+pH+sulph+alcohol, data=wine.df)

## Full Model Summary
summary(full.lm)
AIC(full.lm)

full.res <- rstandard(full.lm)
layout(matrix(c(1,1,2,3), 2, 2, byrow = TRUE))
hist(full.res)
plot(full.res) # By Index
plot(predict(full.lm), full.res) # By Predicted

## Q-Q Plot
qqPlot(full.lm)

## Exhaustive Search 
red.lm <- regsubsets(quality~.,
                     data = wine.df,
                     nbest = 2,
                     nvmax = NULL,
                     force.in = NULL,
                     force.out = NULL,
                     really.big = FALSE,
                     method = "exhaustive")
full.lm.sum <- summary(red.lm)

## R-Squared Search and BIC
ixbest<- seq(1,ncol(wine.df)*2-1,by=2)
par(mfrow=c(1,2))
plot(ixbest,
     full.lm.sum$adjr2[ixbest],
     pch=20,
     xlab="Model",
     ylab="adjR2",
     cex.lab=1,
     cex.axis=1)
plot(ixbest,full.lm.sum$bic[ixbest],
     pch=20,
     xlab="Model",
     ylab="BIC",
     cex.lab=1,
     cex.axis=1)

## Adjusted R-Squared suggested Model
full.lm.sum$outmat[15,] # fx_acidity+vol_acidity+chlorides+pH+sulph+alcohol

## Linear Regression (Reduced: R-Squared)
red.lm <- lm(quality~fx_acidity+vol_acidity+chlorides+
                  pH+sulph+alcohol, data=wine.df)

## Full Model Summary
summary(red.lm)
AIC(red.lm)

## BIC suggested Model
full.lm.sum$outmat[12,] # vol_acidity+chlorides+pH+sulph+alcohol

## Linear Regression (Reduced: BIC)
red.lm <- lm(quality~fx_acidity+vol_acidity+citric_acid+resid_sugar+chlorides+
                  free_sulf_d+tot_sulf_d+density+pH+sulph+alcohol, data=wine.df)

## Full Model Summary
summary(red.lm)
AIC(red.lm)
```

First, we use linear regression in the analysis, since we are wanting to predict the overall quality, or "class" variable. In an attempt to find the most statistically significant variables, we fit the full model and observe each variable's p-values, which are calculated from z-tests. From the coefficient summary, it is clear that there are potential predictors to drop. We should drop "fx_acidity," "citric_acid," "resid_sugar," and "density," since their p-values are relatively large, meaning they are marginally rejected from the model. On the other hand, we should include "vol_acidity," "chlorides," "free_sulf_d," "tot_sulf_d," "pH," "sulph," and "alcohol" in a reduced model, since the p-values are extremely small. Also, it is important to note that the AIC score is 3162, which will be useful for any model comparison made in the future. The R-squared value is fairly low, which indicates that our model does not have much predictive power. Additionally, the residual plots are good, since the residuals are randomly distributed along the x-axis. However, the qq-plot demonstrates some skewness/departure along the left tail, which challenges the normality assumption.

Since there are only 11 variables, we should conduct an exhaustive variable search, while excluding the "quality" variable, since it is essentially the same variable as our response. From the exhaustive search, it seems that the "alcohol" variable has the most explanatory power, followed by the "vol_acidity" variable. The remaining variables compete with each other, but are not as strong as "vol_acidity" and "alcohol." Additionally, it seems that the best model according to the largest R-Squared value is model 15, which contains "fx_acidity," "vol_acidity," "chlorides," "pH," "sulph," and "alcohol." Furthermore, it seems that the best model according to the smallest BIC values is model 12, which contains "vol_acidity," "chlorides," "pH," "sulph," and "alcohol." Clearly, there are overlapping variables, but, more importantly, the "alcohol" and "vol_acidity" variables appear in both models. It's important to note, for future model comparisons, that the AIC values for both models are around 3184.

## Logistic Regression Model

```{r glm}
## Logistic Regression (Full)
full.glm <- glm(class~fx_acidity+vol_acidity+citric_acid+resid_sugar+chlorides+
                  free_sulf_d+tot_sulf_d+density+pH+sulph+alcohol, data=wine.df, family = binomial)

## Full Model Summary
summary(full.glm)

## Residual Plots
full.res <- rstandard(full.glm)
layout(matrix(c(1,1,2,3), 2, 2, byrow = TRUE))
hist(full.res)
plot(full.res) # By Index
plot(predict(full.glm), full.res) # By Predicted

## Logistic Regression (Reduced)
red.glm <- glm(class~vol_acidity+citric_acid+chlorides+
                  free_sulf_d+tot_sulf_d+sulph+alcohol, data=wine.df, family = binomial)

# Reduced Model Summary
summary(red.glm)
```

First, we use logistic regression in the analysis, since we are wanting to predict the overall quality, or "class" variable, which has been binarized in the data transformation section of our analysis. In an attempt to find the most statistically significant variables, we fit the full model and observe each variable's p-values, which are calculated from z-tests. From the coefficient summary, it is clear that there are potential predictors to drop. We should drop "fx_acidity," "resid_sugar," "density," and "pH" since their p-values are relatively large, meaning they are marginally rejected from the model. On the other hand, we should include "vol_acidity," "citric_acid," "chlorides," "free_sulf_d," "tot_sulf_d," "sulph," and "alcohol" in a reduced model, since the p-values are extremely small. The residual plots seem to perform fairly well, since they are randomly distributed between -2 and 2. After comparing the AIC scores between the full model and the reduced model mentioned above, it seems that they both have an AIC score of 1679. The AIC score of reduced model does not improve, but since the model is simpler than the full model, we will decide to prefer the reduced model. Lastly, it seems that we will most likely need to deal with multicollinearity between particularly related variables, such as "free_sulf_d" and "tot_sulf_d" after analysis of the coefficient summary and our previously conducted exploratory analysis. However, we should prefer an approach involving logistic regression predicting "class," rather than an approach involving linear regression predicting "quality," since the logistic regression model has a smaller AIC score.

## Decision Tree

```{r dt}
## Transform Dataset
wine.df.temp <- wine.df[,-c(1, 13)]

## Create Model
model.dt <- rpart(as.factor(class) ~ ., data = wine.df.temp,
                  method = "class")

## Confusion Matrix
pred.dt <- predict(model.dt, wine.df.temp, type = "class")
table(pred.dt, wine.df.temp$class)

## Calculate Accuracy
mean(pred.dt==wine.df.temp$class)

## Perform 10-fold Cross Validation
model <- train(as.factor(class) ~ ., wine.df.temp,
               method = "rpart",
               trControl = trainControl(
                 method = "cv", number = 10,
                 verboseIter = TRUE))

## Print Summary and Confusion Matrix of Cross Validated Model
model
confusionMatrix(predict(model, wine.df.temp), wine.df.temp$class)
```

In R, the Decision Tree classifier is fairly simple to implement when using the "rpart" package. The Decision Tree classifier in this package seems to be widely used and accepted amongst statisticians that use R, as well. It seems that the model only results in an accuracy of 77% on the training dataset, before even performing cross validation. A single iteration results in a True Positive Rate equal to 81%, a True Negative Rate equal to 73%, a False Positive Rate equal to 27% , and a False Negative rate equal to 18%. For validation purposes, we used the "caret" package's Decision Tree classifier to verify our findings, along with performing a 10-fold cross validation, as well. This approach resulted in a somewhat different accuracy measurement: an accuracy of 73%, a True Positive Rate equal to 85%, and a True Negative Rate equal to 59%. Therefore, the initial Decision Tree classifier seemed to have been overfitting the data, since the results from cross-validation were somewhat worse, except for the True Positive Rate, which could be useful for those who are seeking a high True Positive Rate.

## Rules-Based

```{r rb}
## Transform Dataset
wine.df.temp <- wine.df[,-c(1, 13)]

## Create Model
model.rb <- C5.0(as.factor(class) ~ ., data = wine.df.temp)

## Confusion Matrix
pred.rb <- predict(model.rb, wine.df.temp)
table(pred.rb, wine.df.temp$class)

## Calculate Accuracy
mean(pred.rb==wine.df.temp$class)

## Perform 10-fold Cross Validation
model <- train(as.factor(class) ~ ., wine.df.temp,
               method = "C5.0",
               trControl = trainControl(
                 method = "cv", number = 10,
                 verboseIter = TRUE))

## Print Summary and Confusion Matrix of Cross Validated Model
model
```

The Rule-based classifier is fairly simple to implement when using the "C5.0" package. The Rule-based classifier in this package seems to be used and accepted amongst statisticians that use R, as well. The package seems to contain the "C5.0" rule-based classification function that fits classification tree models and rule-based models using Quilan's C5.0 algorithm. Seemingly, the model results in an accuracy of 86% on the training dataset, before even performing cross validation. A single iteration results in a True Positive Rate equal to 87%, a True Negative Rate equal to 85%, a False Positive Rate equal to 15% , and a False Negative rate equal to 13%. For validation purposes, we used the "caret" package's Rule-based classifier to verify our findings, along with performing a 10-fold cross validation, as well. This approach resulted in different predictive measurements: an accuracy of 78%, a True Positive Rate equal to 77%, and a True Negative Rate equal to 78%. Therefore, the initial Rule-based classifier seemed to have been overfitting the data, since the results from cross-validation were somewhat worse.

## Naive Bayes

```{r nb}
## Transform Dataset
wine.df.temp <- wine.df[,-c(1, 13)]

## Create Model
model.nb <- naiveBayes(as.factor(class) ~ ., data = wine.df.temp)

## Confusion Matrix
pred.nb <- predict(model.nb, wine.df.temp)
table(pred = pred.nb, true=wine.df.temp$class)

## Calculate Accuracy
mean(pred.nb==wine.df.temp$class)

## Perform 10-fold Cross Validation
model <- train(as.factor(class) ~ ., wine.df.temp,
               method = "nb",
               trControl = trainControl(
                 method = "cv", number = 10,
                 verboseIter = TRUE))

## Print Summary of Cross Validated Model
model
```

The Naive Bayes algorithm is easy to implement when using the "e1071" package. The Naive Bayes algorithm in this package seems to be widely used and accepted amongst statisticians that use R, as well. Although the implementation is simple, the model only results in an accuracy of 73% on the training dataset, before even performing cross validation. A single iteration results in a True Positive Rate equal to 75%, a True Negative Rate equal to 72%, a False Positive Rate equal to 28% , and a False Negative rate equal to 25%. For validation purposes, we used the "caret" package's Naive Bayes algorithm to verify our findings, and to perform a 10-fold cross validation. This approach resulted in a similar accuracy measurement: a True Positive Rate equal to 74% and a True Negative Rate equal to 73%.

## Artificial Neural Network

```{r nn}
## Transform Dataset
wine.df.temp <- wine.df[,-c(1, 13)]

## Normalize Data
maxs <- apply(wine.df.temp, 2, max) 
mins <- apply(wine.df.temp, 2, min)
scaled <- as.data.frame(scale(wine.df.temp, center = mins, scale = maxs - mins))
train_ <- scaled[1:1118,]
test_ <- scaled[1119:1597,]

## Create Model
n <- names(train_)
f <- as.formula(paste("class ~", paste(n[!n %in% "class"], collapse = " + ")))
model.nn <- neuralnet(f, data=train_, linear.output=TRUE)

# Scale the Model Back
pr.nn <- compute(model.nn,test_[,-12])
pr.nn_ <- pr.nn$net.result*(max(wine.df.temp$class)-min(wine.df.temp$class))+min(wine.df.temp$class)
test.r <- (test_$class)*(max(wine.df.temp$class)-min(wine.df.temp$class))+min(wine.df.temp$class)

## Calculate MSE Error
MSE.nn <- sum((test.r - pr.nn_)^2)/nrow(test_)
MSE.nn

## Calculate Confusion Matrix
pr.nn <- round(compute(model.nn,test_[,-12])$net.result)
table(pr.nn, test_$class)

## Perform 10-fold Cross Validation
model <- train(as.factor(class) ~ ., wine.df.temp,
               method = "nnet",
               trControl = trainControl(
                 method = "cv", number = 10,
                 verboseIter = TRUE))

## Print Summary of Cross Validated Model
model
```

The Neural Network classifier is somewhat difficult to implement, but is considerably easier to use with the "neuralnet" package. In this package, the Neural Network classifier seems to be widely used and accepted amongst statisticians that use R, as well. The model results in an MSE of 0.18, and an accuracy of 75% on the training dataset, before even performing cross validation. A single iteration results in a True Positive Rate equal to 77%, a True Negative Rate equal to 72%, a False Positive Rate equal to 28% , and a False Negative rate equal to 23%. For validation purposes, we used the "caret" package's Artifical Neural Network algorithm to verify our findings, and to perform a 10-fold cross validation. This approach resulted in a slightly worse accuracy measurement: an average accuracy equal to 70%.

## Support Vector Machine

```{r vm}
## Transform Dataset
wine.df.temp <- wine.df[,-c(1, 13)]

## Create Model
model.vm <- svm(as.factor(class) ~ ., data = wine.df.temp)

## Confusion Matrix
pred.vm <- predict(model.vm, wine.df.temp)
table(pred = pred.vm, true=wine.df.temp$class)

## Calculate Accuracy
mean(pred.vm==wine.df.temp$class)

## Perform 10-fold Cross Validation
model <- train(as.factor(class) ~ ., wine.df.temp,
               method = "svmLinearWeights2",
               trControl = trainControl(
                 method = "cv", number = 10,
                 verboseIter = TRUE))

## Print Summary of Cross Validated Model
model
```

In R, the Support Vector Machine classifier is easy to implement when using the "e1071" package. In this package, the Support Vector Machine function seems to be widely used and accepted amongst statisticians that use R, as well. The model results in an accuracy of 80% on the training dataset, before even performing cross validation. A single iteration results in a True Positive Rate equal to 82%, a True Negative Rate equal to 77%, a False Positive Rate equal to 23% , and a False Negative rate equal to 18%. For validation purposes, we used the "caret" package's implementation of a Support Vector Machine classifier to verify our findings, and to perform a 10-fold cross validation. This approach resulted in a similar accuracy measurement: an accuracy equal to 69%. This implies that our model fit during the preliminary analysis on the testing data contained a high level of overfitting.

## Ensemble Learner using Random Forest

```{r rf}
## Transform Dataset
wine.df.temp <- wine.df[,-c(1, 13)]

## Create Model
model.rf <- randomForest(as.factor(class) ~ ., data = wine.df.temp)

## Confusion Matrix
pred.rf <- predict(model.rf, wine.df.temp)
table(pred = pred.rf, true=wine.df.temp$class)

## Calculate Accuracy
mean(pred.vm==wine.df.temp$class)

## Perform 10-fold Cross Validation
model <- train(as.factor(class) ~ ., wine.df.temp,
               method = "rf",
               trControl = trainControl(
                 method = "cv", number = 10,
                 verboseIter = TRUE))

## Print Summary of Cross Validated Model
model
```
In R, the Random Forest function is fairly simple to implement when using the "randomForest" package. In this package, the Random Forest algorithm seems to be widely used and accepted amongst statisticians that use R, as well. Before performing cross-validation, the model resulted in an accuracy of 99% on the training dataset, which certainly indicates a high level of overfitting. The single iteration results in a True Positive Rate equal to 100%, a True Negative Rate equal to 98%, a False Positive Rate equal to 2% , and a False Negative rate equal to 0%. For validation purposes, we used the "caret" package's implementation of the Random Forest function to verify our findings, and to perform a 10-fold cross validation in order to avoid overfitting. This approach resulted in a very different accuracy measurement, but the best accuracy measurement in comparison to the other classifiers. The average accuracy equaled to 83%.

## Conclusion

In order to avoid overfitting throughout any analysis, cross-validation was key in iterating over testing and training data to provide the most correct predictive measurements. After cross-validation, the Decision Tree classifier had an accuracy measurement equal to 73%. This accuracy measurement had potential for improvement, especially when comparing the measurement to other predictive measurements from different classifiers. For example, the Rule-based classifier had an accuracy of 78%. Although the Rule-based classifier had a better accuracy measurement, the majority of other tested classifiers had a worse accuracy measurement in comparison to the Decision Tree classifier, after performing cross-validation. The Naive Bayes classifier produced an accuracy of 73%, the Neural Network classifier had an accuracy equal to 70%, and the Support Vector Machine had the worst accuracy measurement: an accuracy of 69%. However, the classifier with the best accuracy measurement was the Random Forest classifier, which had an accuracy of 83% after cross-validation. Therefore, the Random Forest classifier should generally be used for predictive purposes, since it seems to possess the greatest predictive power, generally. Although these classifiers generally did not produce very good accuracy measurements, some classifier could be preferred over others, and even the Random Forest classifier, depending on the goal during analysis. For example, someone could arguably prefer the Decision Tree classifier if the goal is to predict the highest True Positive Rate, since the Decision Tree classifier supports the greatest True Positive Rate. Essentially, the preferred classifier can be modified or switched for another, depending on the overarching goal throughout the analysis.

